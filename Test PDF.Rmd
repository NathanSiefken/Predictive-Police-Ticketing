---
title: "Test PDF"
author: "Nathan Siefken"
date: "2/3/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### Background and Motivation *



### Data Set*

### Goal *


The goal of my project is do to a deep analysis of the data and determine patterns as a human then independently create different machines that will learn from the data these patterns. To create the machines I will train each machine learning algorithm from the  training subset to predict the amount of parking citation depending on the time of day and the day of the week in Los Angeles.

To do this, I will first wrangle the data and then I will do some exploratory data analysis. Once  can see patterns as a human, I will create a baseline model which I will compare my two methods of machine learning, Linear Regression and Regression Tree and ultimately determine if they would see the same patters I did in my analysis which will prove what machine learning is.

During analysis we will review the Root Mean Squared Error or RMSE as a guide, to determine the best method. RMSE allows us the flexibility to compare the results from different types of models, Linear Regression and Regression Tree. Kaggle competitions also use RMSE or MAE as the metric for judging competitions. I will keep track of the RMSE in a matrix for ease of comparison in the results section.

### Data Loading and Setup

We will utilize and load several packages from CRAN to assist with our analysis. These will be automatically downloaded and installed during code execution. 

I will do some data analysis exploration and then before get into the machine learning section, I will split the data into a training and validation set (20%).



```{r, warning=FALSE}
# This code chunk simply makes sure that all the libraries used here are installed. 
packages <- c("knitr","dplyr",  "tidyr", "caret", "ggplot2", "caret",
              "plotly","lubridate","leaflet", "stringr","rpart.plot", "rpart")
if ( length(missing_pkgs <- setdiff(packages, rownames(installed.packages()))) > 0) {
message("Installing missing package(s): ", paste(missing_pkgs, collapse = ", "))
install.packages(missing_pkgs)
}

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading libraries
library(tidyr) 
library(dplyr)
library(ggplot2)
library(lubridate) # for working with dates
library(plotly) # for interactive plots
library(janitor)
library(leaflet) # Geomapping
library(colorRamps)
library(proj4)
library(validate)
library(stringr)
library(rpart)
library(rpart.plot)
library(caret)
library(knitr)
FTP<- read.csv("FTP.csv", stringsAsFactors = FALSE)
```
## Intro: 


## Cleaning our Data

We will combine the Issue.Date and the Issue.Time into its own column. To do this we will need to restructure the data into a time series friendly format.

Currently there is an incorrect time stamp of "T00:00:00" Issue.date variables that needs to be removed.

```{r}
#Removing excess information
FTP$Issue.Date <- sub("T.*", "", FTP$Issue.Date)
```


We will use some string processing techniques to clean up our Issue.Time column.

```{r pressure, echo=FALSE}
#Now to put our time into a format that we can use
FTP$Issue.time<-sub("(\\d*)(\\d{2})", "\\1:\\2", FTP$Issue.time) # put in delimitter
# The single digit strings were missed, so this code will convert them
FTP$Issue.time <- str_replace(FTP$Issue.time, "^([0-9])$",":0\\1")
# Now we will need to pad our times with a 0 before the ":" for all data that was less than 1:00
FTP$Issue.time <- sprintf("%04s", FTP$Issue.time)
```
In addition, there are Longitude and Latitude variables that have a default value of 99999 that will need to be removed, so that we can plot the coordinates of the tickets.
```{r}
#We can notice that there is a default of 99999 when a coordinate isn't entered.
#We will remove these
FTP<- FTP %>%
  filter(Latitude != 99999) 
```

The following code converts the coordinates from US feet to Longitude and Latitude coordinates. Then we remove the coordinate columns that are in US feet.
```{r}

summary(FTP[9:10])# Our Before
#Create projection element to convert from US Feet coordinates to normal lat lon
pj <- "+proj=lcc +lat_1=34.03333333333333 +lat_2=35.46666666666667 +lat_0=33.5 +
lon_0=-118 +x_0=2000000 +y_0=500000.0000000002 +ellps=GRS80 +datum=NAD83 +
to_meter=0.3048006096012192 no_defs"

#Add converted latitude longitude to FTP dataframe
FTP<- cbind(FTP, data.frame(project(data.frame(FTP$Latitude, FTP$Longitude),
                                    proj = pj, inverse = TRUE)))
FTP <- FTP[-9:-10] #This removes the Latitude and Logitude in Feet from our table
names(FTP)[c(9, 10)] <- c('Longitude', 'Latitude') #Rename column names of converted 
#longitude latitude. Now our data is looking clean and usable
summary(FTP[9:10]) # Our After
```

After all the cleaning of our data, now we can combine Issue.Date and Issue.Time, so we can easily seperate out the Days of the week and Hours of the day and place them into their own column. It will become clear why I would do this further in the report.

```{r}
#combined the date and time
FTP$Date <- as.POSIXlt(paste(FTP$Issue.Date, FTP$Issue.time), format="%Y-%m-%d %H:%M")
#Seperate the days of the week tickets where given and place it into a column
FTP$Weekdays <- weekdays(FTP$Date)
#Seperate the Hours of the day tickets where given and place it into a column
FTP$Hour <- FTP$Date$hour
```

Finally, there are some NAs that we need to eliminate. Since there are so few, simply removing them will not affect our results.
```{r}
FTP <- na.omit(FTP)# Very few for this many observation (less than 1%)
FTP <- FTP[-11] #We will remove our date column because we are
#not able to preform some calculations when a column is in POSIXlt format.
#Since we have no further use for it, we shall remove it.
summary(FTP)  # This is our sqeaky clean table
```

##Analysing the data
First, lets see how much revenue was generated last month just from ticketing.
```{r}
revenue <- FTP %>% summarize(Revenue = sum(Fine.amount))
revenue %>% knitr::kable()
```


Filter top 10 Violations
```{r}
TopViolations <- FTP %>% 
  group_by(Violation.Description) %>% 
  tally() %>% 
  arrange(-n) %>% 
  head(10)

TopViolations %>% knitr::kable()
```


Now, lets graph top 10 Violations for the past month.

```{r}
TopViolationsLastYears <- FTP %>% 
  filter(Violation.Description %in%
           TopViolations$Violation.Description)

p <- ggplot(TopViolationsLastYears, aes(Issue.Date)) + 
  geom_bar(aes(fill=Violation.Description), stat='count')+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#Plot the data), stat='count')
p
```
The lions share of the tickets are "NoPark/Street Clean", also, there apears to be some peaks and valleys. We will investigate further to understand this pattern a little better.

```{r}
#This one would be better for a month
DailyParkingViolation <- FTP %>%
  group_by(Issue.Date) %>%
  tally() %>%
  ggplot(aes(x=Issue.Date, y=n)) +
  geom_point()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

DailyParkingViolation
```

It appears there is a cloud of data points towards the top and the bottom of the graph. That is interesting, it appears that about for every five that are in the upper cloud, two points follow in the lower part of the graph. This would suggest to me that the points on the upper part of the graph are weekdays and the lower part are weekends. 

The following code will show a table of frequency of tickets per day of the week and in fact this confirms our theory.

```{r}
table(FTP$Weekday) %>% knitr::kable()
```

We aren't going to stop at the day of the week, we should investigate the time of day. In case you are not familiar with the 24-hour clock, 0 is midnight that runs to 23:59 (11:59 PM).
```{r}
table(FTP$Hour)
```

Tables are interesting to look at but a visualization will make it much easier to interpret.
```{r}
WeekdayCounts = as.data.frame(table(FTP$Weekday))

ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1))
```

To make the graph easier to read we will label the x and y axis and put the days of the week in order.
```{r}
WeekdayCounts$Var1 = factor(WeekdayCounts$Var1, ordered=TRUE,
                            levels=c("Sunday","Monday", "Tuesday","Wednesday",
                                     "Thursday", "Friday","Saturday")) 
#We can change the Var1 variable to be an ordered factor variable
ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1)) +
  xlab("Day of the Week") + ylab("Total Ticket  Given Out")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

```
Wow, it looks like the most predictive ticketing day for the LA police is Wednesday. Now, let’s add the hour of the day and visualize any patterns. 

First, I'll create a frequency table for the weekday and hour and then we will put it into a data frame. The I'll visualize it with a line graph.

```{r}
# This will create our table
table(FTP$Weekday, FTP$Hour) %>% knitr::kable()
#We will save this as a data frame 
DayHourCounts = as.data.frame(table(FTP$Weekday, FTP$Hour))
# Convert the second variable, Var2, to numbers and call it Hour:
DayHourCounts$Hour = as.numeric(as.character(DayHourCounts$Var2))
# Create out plot:
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1))
```

Some color will make this graph a lot informative
```{r}
# Fix the order of the days and add color
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1,
                                                           color=Var1), size=2)

```

There is a distinction in the frequency tickets are given on weekdays vs weekends. Let's separate the two and visualize. 
```{r}
DayHourCounts$Type = ifelse((DayHourCounts$Var1 == "Sunday") |
                              (DayHourCounts$Var1 == "Saturday"),
                            "Weekend", "Weekday")
# Redo our plot, this time coloring by Type:

ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + 
  geom_line(aes(group=Var1,color=Type), size=2,alpha=0.5) 

```

Another way to interpret this information is with a heatmap:
```{r}
# Fix the order of the days:
DayHourCounts$Var1 = factor(DayHourCounts$Var1, ordered=TRUE,
                            levels=c("Monday", "Tuesday", "Wednesday",
                                     "Thursday", "Friday", "Saturday", "Sunday"))
#Change the label on the legend, and get rid of the y-label:
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq)) +
  scale_fill_gradient(name="Total Tickets Given") + 
  theme(axis.title.y = element_blank())
```
The dark blue is the low frequency and the light blue is the high frequency but let’s give this heat map some heat and give and change the color scheme to red and white. Also, I will label the key.
```{r}
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq)) +
  scale_fill_gradient(name="Total Tickets", low="white", high="red") +
  theme(axis.title.y = element_blank())
```
The heatmap confirms that 8am, 10, and 12 am are the most popular times to get a ticket

# Machine Learning Methods
It is always best practice to split your data into 
Before we get started I wanted to re-label the variables so that machine learning output will be easier to interpret. 
```{r}

DayHourCounts <- setNames(DayHourCounts, c("Weekday","Hour","Frequency",
                                           "HourAsNumber", "Weekday/Weekend"))
#
set.seed(21)
test_index <- createDataPartition(y = DayHourCounts$Frequency, times = 1,
                                  p = 0.2, list = FALSE)
my_train <- DayHourCounts[-test_index,]
test <- DayHourCounts[test_index,]
```

The following code generates a function that will calculate the RMSE for actual values (true_ratings) from our test set to their corresponding predictors from our models:

   \[
    \makebox[\linewidth]{$RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(\frac{d_i -f_i}{\sigma_i}\Big)^2}}$}
    \]
    
```{r}
RMSE <- function(true_Frequency, predicted_Frequency){
  sqrt(mean((true_Frequency - predicted_Frequency)^2))
}
```

#Baseline Model

We will start with our baseline, the most basic prediction model. This is the average for all hours across all days and use this average to predict our ratings.

```{r}
mu_hat <- mean(my_train$Frequency)
mu_hat
```

Now that we have our $\hat{\mu}$ we can determine RMSE for our baseline method. 

```{r}
Baseline_rmse <- RMSE(test$Frequency, mu_hat)
Baseline_rmse

```

We are getting a RMSE of about 656. Our prediction is on average 656 of citation off the actual amount of citations that are given for each day. This is the case for a couple of reasons, there seems to be days of very high amounts and then days that are very low tickets that are given. However, for this model this is the lowest RMSE we can have. To demonstrate we will use 1000 and 750 to prove that we will get a larger RMSE with another value than our baseline. 

```{r}
predictions <- rep(1000, nrow(test))
RMSE(test$Frequency, predictions)

```
RMSE of 725.29 is worse than our baseline.

The following code will keep track with all of our test with this matrix.

```{r}
rmse_results <- data_frame(method = "Baseline", RMSE = Baseline_rmse)
rmse_results %>% knitr::kable()
```

#Linear Regression

Now lets run a linear regression model to see if we can improve on our baseline model

  \[
    \makebox[\linewidth]{${y}_{u,i}=\mu+b_i+\varepsilon_{u,i}$}
    \]

```{r}
LR = lm(Frequency ~ Weekday + Hour, data=my_train) 
summary(LR)
```

```{r}
LR_pred <- predict(LR, newdata=test)

LR_rmse <- RMSE(test$Frequency, LR_pred)
LR_rmse
```
If you happen to be more familiar with the Sum Square Errors or SSE for validation. I wanted to provide you the following code to validate my RMSE metric, as well as the RMSE function.

```{r}
LR_sse <- sum((LR_pred- test$Frequency)^2)
LR_sse
RMSE <- sqrt(LR_sse/nrow(test))
RMSE
```

When in math, 
facts are facts,
when in doubt,
prove your math :)

```{r}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Linear Regression Model",  
                                     RMSE = LR_rmse ))
rmse_results %>% knitr::kable()
```



##Regression Tree
<p>
Lets se if we can further improve this with a regression tree or CART model
```{r}
tree = rpart(Frequency~ Weekday + Hour, data=my_train) 
prp(tree)
```

```{r}
tree_pred = predict(tree, newdata=test)
tree_sse = sum((tree_pred - test$Frequency)^2)
tree_sse
```
\pagebreak

#Results
```{r}
Tree_rmse <- sqrt(tree_sse/nrow(test))
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regression Tree Model",  
                                     RMSE = Tree_rmse ))
rmse_results %>% knitr::kable()
```


#Conclusion:

What I really enjoyed about this project was that I able to do some extensive exploratory data analysis to know the patterns that I hope my machine would realize. Then, I was able to build different machines and test their predictions. I could then determine which was the best method for this project. Ultimately the beauty of this project, was finding out that my machine would learn from the data and see pattern from this data and learn. With this validation you can know that no matter how patterns change in parking ticket citations, the machine will learn and adapt.


Some further insights : 

I really enjoyed some other tools in R that I'm unable to display in a PDF because they are reactive is a cluster map and a heatmap. I added some screan shots below because I thought it was worth sharing. (I plan to put some pics of the reactive maps and maybe create a quick shiny app and place a link for them to click onto.)




